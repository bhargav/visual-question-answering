{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import argparse\n",
    "\n",
    "import lessdummy1 as utilities\n",
    "import cocoIDToFeatures as cocoImageUtils\n",
    "\n",
    "tfile = '../features/coco_vgg_IDMap.txt'\n",
    "\n",
    "args = {}\n",
    "args['answer_vector_file']='answer_feature_list.json'\n",
    "args['glove_file']='../glove/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading GloVE and VGG raw files\n",
      "Reading the data and creating features\n"
     ]
    }
   ],
   "source": [
    "print \"Reading GloVE and VGG raw files\"\n",
    "\n",
    "glove_word_vec_file = args['glove_file']\n",
    "word_vec_dict = utilities.readGloveData(glove_word_vec_file)\n",
    "\n",
    "imageDict = cocoImageUtils.generateDictionary(tfile)\n",
    "feats = sio.loadmat('./../features/coco/vgg_feats.mat')['feats']\n",
    "\n",
    "print \"Reading the data and creating features\"\n",
    "\n",
    "answer_vector_file = open(args['answer_vector_file'], 'r')\n",
    "answerFeatureVector = json.loads(answer_vector_file.read())\n",
    "\n",
    "answer_vector_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:18.065623\n",
      "creating index...\n",
      "index created!\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:23.249883\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './../VQA/PythonHelperTools')\n",
    "from vqaTools.vqa import VQA\n",
    "\n",
    "dataDir = './../VQA'\n",
    "taskType = 'MultipleChoice'\n",
    "dataType = 'mscoco' # 'mscoco' for real and 'abstract_v002' for abstract\n",
    "dataSubType = 'train2014'\n",
    "annFile = '%s/Annotations/%s_%s_annotations.json' % (dataDir, dataType, dataSubType)\n",
    "quesFile = '%s/Questions/%s_%s_%s_questions.json' % (dataDir, taskType, dataType, dataSubType)\n",
    "imgDir = '%s/Images/%s/%s/' % (dataDir, dataType, dataSubType)\n",
    "vqaTrain = VQA(annFile, quesFile)\n",
    "dummyano = vqaTrain.dataset['annotations']\n",
    "answerFeatures = utilities.createAnswerFeatures(dummyano)\n",
    "\n",
    "vqaVal = VQA(annFile, quesFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "numQuestions = 1000\n",
    "q = 0\n",
    "for quesID, annotation in vqaVal.qa.iteritems():\n",
    "    q += 1\n",
    "    if q == numQuestions:\n",
    "        break\n",
    "        \n",
    "    question = vqaVal.qqa[quesID]\n",
    "    question_text = question['question'].strip().replace('?', ' ?').split()\n",
    "    imgID = annotation['image_id']\n",
    "    ansString = annotation['multiple_choice_answer']\n",
    "    \n",
    "    dataset.append({'question': question_text, 'answer': ansString, 'image': imgID})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 237, 7: 206, 8: 177, 5: 133, 9: 98, 10: 54, 11: 30, 4: 27, 12: 17, 13: 9, 15: 4, 14: 3, 16: 2, 17: 2})\n",
      "Max Question Length =  17\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter([len(x['question']) for x in dataset])\n",
    "maxlen = max(c.keys())\n",
    "print c\n",
    "print \"Max Question Length = \", maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxlen = 23\n",
    "nb_train = len(dataset)\n",
    "nb_timestep = maxlen + 1 # For Image Vector\n",
    "word_vec_dim = len(word_vec_dict['hi'])\n",
    "image_dim = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LSTM Model###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the LSTM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Reshape, Merge, RepeatVector\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "inner_layer_size = 512\n",
    "output_size = 1000\n",
    "input_size = word_vec_dim\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "imageModel = Sequential()\n",
    "imageModel.add(Dense(input_size, input_shape=(4096,)))\n",
    "imageModel.add(Dropout(0.2))\n",
    "imageModel.add(RepeatVector(maxlen))\n",
    "\n",
    "questionModel = Sequential()\n",
    "questionModel.add(Reshape(input_shape=(maxlen, input_size,), dims=(maxlen, input_size,)))\n",
    "\n",
    "model.add(Merge([imageModel, questionModel], mode='concat', concat_axis=2))\n",
    "\n",
    "model.add(GRU(inner_layer_size, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(output_size, init='uniform', activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "                    \n",
    "model.add(Dense(output_size, init='uniform', activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils.visualize_util import to_graph\n",
    "\n",
    "# SVG(to_graph(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating X_train and Y_train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.zeros(shape=(nb_train, maxlen, input_size))\n",
    "Image_train = np.zeros(shape=(nb_train, 4096))\n",
    "Y_train = np.zeros(shape=(nb_train, len(answerFeatureVector)) , dtype='bool')\n",
    "\n",
    "idx = 0\n",
    "for item in dataset:\n",
    "    q = item['question']\n",
    "    padding = maxlen - len(q)\n",
    "    for i in xrange(padding):\n",
    "        X_train[idx, i, :] = np.zeros(input_size)\n",
    "        \n",
    "    for word in q:\n",
    "        X_train[idx, padding, :] = utilities.getWordVector(word, word_vec_dict)\n",
    "    Y_train[idx, :] = utilities.getAnswerVector(item['answer'], answerFeatureVector)\n",
    "    \n",
    "    Image_train[idx, :] = np.asarray(feats[:, imageDict[item['image']]])\n",
    "    \n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 899 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "899/899 [==============================] - 7s - loss: 5.9973 - acc: 0.1691 - val_loss: 4.7934 - val_acc: 0.0400\n",
      "Epoch 2/5\n",
      "899/899 [==============================] - 7s - loss: 4.6651 - acc: 0.2058 - val_loss: 4.9418 - val_acc: 0.3600\n",
      "Epoch 3/5\n",
      "899/899 [==============================] - 7s - loss: 4.4826 - acc: 0.2603 - val_loss: 4.8339 - val_acc: 0.1800\n",
      "Epoch 4/5\n",
      "899/899 [==============================] - 7s - loss: 4.4706 - acc: 0.2925 - val_loss: 4.4834 - val_acc: 0.1800\n",
      "Epoch 5/5\n",
      "899/899 [==============================] - 7s - loss: 4.2996 - acc: 0.2925 - val_loss: 4.5947 - val_acc: 0.3500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e1627810>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Image_train, X_train], Y_train, nb_epoch=5, validation_split=0.1, show_accuracy=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"layers\": [{\"layers\": [{\"layers\": [{\"b_constraint\": null, \"name\": \"Dense\", \"activity_regularizer\": null, \"W_constraint\": null, \"input_shape\": [4096], \"init\": \"glorot_uniform\", \"activation\": \"linear\", \"input_dim\": null, \"b_regularizer\": null, \"W_regularizer\": null, \"output_dim\": 300}, {\"p\": 0.2, \"name\": \"Dropout\"}, {\"name\": \"RepeatVector\", \"n\": 23}], \"name\": \"Sequential\"}, {\"layers\": [{\"dims\": [23, 300], \"name\": \"Reshape\", \"input_shape\": [23, 300]}], \"name\": \"Sequential\"}], \"mode\": \"concat\", \"dot_axes\": -1, \"name\": \"Merge\", \"concat_axis\": 2}, {\"name\": \"LSTM\", \"inner_activation\": \"hard_sigmoid\", \"go_backwards\": false, \"output_dim\": 512, \"stateful\": false, \"init\": \"glorot_uniform\", \"inner_init\": \"orthogonal\", \"input_dim\": 600, \"return_sequences\": false, \"activation\": \"tanh\", \"forget_bias_init\": \"one\", \"input_length\": null}, {\"p\": 0.2, \"name\": \"Dropout\"}, {\"b_constraint\": null, \"name\": \"Dense\", \"activity_regularizer\": null, \"W_constraint\": null, \"init\": \"uniform\", \"activation\": \"tanh\", \"input_dim\": null, \"b_regularizer\": null, \"W_regularizer\": null, \"output_dim\": 1000}, {\"p\": 0.2, \"name\": \"Dropout\"}, {\"b_constraint\": null, \"name\": \"Dense\", \"activity_regularizer\": null, \"W_constraint\": null, \"init\": \"uniform\", \"activation\": \"softmax\", \"input_dim\": null, \"b_regularizer\": null, \"W_regularizer\": null, \"output_dim\": 1000}], \"loss\": \"categorical_crossentropy\", \"theano_mode\": null, \"name\": \"Sequential\", \"class_mode\": \"categorical\", \"optimizer\": {\"epsilon\": 1e-06, \"lr\": 0.0010000000474974513, \"name\": \"RMSprop\", \"rho\": 0.8999999761581421}}\n"
     ]
    }
   ],
   "source": [
    "print model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
